# Basic Alertmanager and Prometheus rules for QMTL metrics
# For a complete list of suggested alerts see docs/operations/monitoring.md
alertmanager:
  route:
    receiver: 'slack'
  receivers:
    - name: 'slack'
      slack_configs:
        - channel: '#alerts'
          send_resolved: true

prometheus:
  groups:
    - name: qmtl
      rules:
        - alert: DiffDurationHigh
          expr: diff_duration_ms_p95 > 200
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Diff processing slow
        - alert: DiffFailureRateHigh
          expr: sum(rate(diff_failures_total[5m])) / clamp_min(sum(rate(diff_requests_total[5m])), 1e-6) > 0.05
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: DAG diff failure rate above 5%
        - alert: DiffThroughputLow
          expr: sum(rate(diff_requests_total[5m])) < 0.01
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: DAG diff throughput below 0.6/min
        - alert: NodeCacheMemoryHigh
          expr: nodecache_resident_bytes{node_id="all",scope="total"} > 5e9
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: NodeCache memory usage high
        - alert: QueueLagHigh
          expr: queue_lag_seconds > queue_lag_threshold_seconds
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Kafka consumer lag above threshold
        - alert: ComputeNodeCountHigh
          expr: compute_nodes_total > 50000
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: ComputeNode count growth is abnormal
        - alert: QueueCountHigh
          expr: queues_total > 100000
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: Queue count growth is abnormal
        - alert: SentinelSkewHigh
          expr: sentinel_skew_seconds > 5
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Sentinel traffic skew persists beyond 5s
        - alert: CrossContextCacheHit
          expr: cross_context_cache_hit_total > 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Cross-context cache hit detected (SLO=0)
            runbook: docs/operations/monitoring.html#runbook-cross-context-cache-hits-slo-0
        - alert: WorldApplyFailureDetected
          expr: sum by (world_id) (increase(world_apply_failure_total[5m])) > 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Apply run failures detected for a world
        - alert: WorldApplyFailureRateHigh
          expr: |
            sum(rate(world_apply_run_total{status="failure"}[5m])) / clamp_min(
              sum(rate(world_apply_run_total{status!="started"}[5m])),
              1e-6
            ) > 0.05
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: Apply failure rate exceeded 5% over 10m
        - alert: WorldAllocationSnapshotStale
          expr: max_over_time(world_allocation_snapshot_stale_ratio[5m]) > 0.1
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: Allocation snapshot stale ratio above 10%
        - alert: ControlBusApplyAckLatencyHigh
          expr: max_over_time(controlbus_apply_ack_latency_ms{phase="freeze"}[5m]) > 5000
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Apply freeze acknowledgements are slower than 5s
        - alert: SeamlessSla99thDegraded
          expr: histogram_quantile(0.99, sum by (le) (rate(seamless_sla_deadline_seconds_bucket{phase="total"}[5m]))) > 240
          for: 5m
          labels:
            severity: warning
            service: seamless
          annotations:
            summary: Seamless SLA 99th percentile exceeded 240s for 5 minutes
            runbook: docs/operations/seamless_sla_dashboards.html
        - alert: SeamlessBackfillStuckLease
          expr: min_over_time(backfill_completion_ratio[10m]) < 0.5
          for: 10m
          labels:
            severity: critical
            service: seamless
          annotations:
            summary: Seamless backfill lease stuck below 50% completion
            runbook: docs/operations/seamless_sla_dashboards.html#runbooks
        - alert: SeamlessConformanceFlagSpike
          expr: sum(increase(seamless_conformance_flag_total[15m])) > 50
          for: 15m
          labels:
            severity: warning
            service: seamless
          annotations:
            summary: Surge in Seamless conformance flags over the last 15 minutes
            runbook: docs/operations/seamless_sla_dashboards.html#validation-tooling
